---
title: "Time series analysis, 7.5 hp "
subtitle: "Inlämningsuppgift 2, 2.5 hp"
author: 
- Sverker Berggren
- Love Alfredsson
- Emil Möller
date: last-modified
format: 
  html:
    self-contained: true
  #pdf: default
language: 
  title-block-author-single: " "
crossref-fig-title: "Figur"
title-block-banner-color: Primary
title-block-published: "Publicerad"
author-title: "Gruppmedlemmar"
title-block-banner: true
editor: source
---

```{r}
#| output: false
#| echo: false
library(dplyr)
library(ggplot2)
library(tidyverse)
library(jsonlite)
library(lubridate)
library(stats)
library(tseries)
library(vars)
library(portes)
```
# Abstract
In this report will we investigate the relationship between commodity future prices and the price of Dow Inc, on of the larger plastic producers. The question we have if there is any material connection, and if so is it strong enough to incorporate into a potential trading strategy that could generate a meaningful alpha?

# Introduction
The choice of company is Dow Chemical Company, or its stock designation DOW, is based on this article from Plastics Technology https://www.plastics-technology.com/articles/top-largest-plastic-producing-companies. Here DOW is said to be the biggest plastic producing company, and as such we assume that it is proportionally more resilient to individual company management and is more representative of the plastic industry as a whole. Looking at their Q3 report for 2025 we can see that Packaging & Specialty Plastics account for a bit over half of the companys overall revenue. https://investors.dow.com/en/news/news-details/2025/Dow-reports-third-quarter-2025-results/default.aspx. A change in the margins of the plastic production therefore should have a big impact on the price evaluation of the company.

We then choose which commodities to take into account. There is no concrete explanation from DOW regarding what commodities they source for their plastic production so we used the article from U.S Energy Information Administration to try to find which commodities are the most relevant for plastic production. https://www.eia.gov/tools/faqs/faq.php?id=34&t=6. Here we found that materials refined from crude oil are the most used feedstocks and that byproducts from natural gas production are also important. The article stresses that there are no hard numbers to give regarding the exact feedstocks used so it is a bit speculative regarding how it actually looks like in practice. With this information in mind we choose Crude Oil futures and american natural gas futures as our commodities of interest. 

# Description of data

The stock data is sourced from Alpaca markets api and the future data is sourced from Databento's service. The futures we have queryed are CL, Crude Oil futures and NG, Henry Hub Natural Gas Futures. DOW started trading in its current incarnation 2019 April 2 so our timeframe is between 2019 April 2 - 2026 Janury 4. We choose to analyze full day aggregations of the financial instruments.        

```{r}
stock_data <- fromJSON("data/dow_stock_price.json")

dow_price <- stock_data$DOW  %>%   as_tibble()

dow_price = dow_price %>% rename(high = h, open = o, close = c,low = l,date = t,volume = v, number_of_trades = n,volume_weighted_average_price = vw)
dow_price = dow_price %>% mutate(date = as.Date(date))
futures_price <- stream_in(file("data/glbx-mdp3-20190402-20260103.ohlcv-1d.json"),verbose = FALSE)%>% unnest_wider(hd) %>% as_tibble()
```

```{r}
futures_price = futures_price %>%
  mutate(
    ts_event = as.Date(ts_event),
    open   = as.numeric(open),
    high   = as.numeric(high),
    low    = as.numeric(low),
    close  = as.numeric(close),
    volume = as.numeric(volume)
  )
```

Firstly we can explain the terminology regarding market bars. High means the highest price the instrument traded for during the specified timeframe. Low is conversly the lowest price traded for. Close is what the price was when the time frame ended. Volume is the number of financial instruments traded while the number of trades is the number of transactions. A transaction can buy or sell a variable amount of the actual instrument. Volume weighted average price is the volume traded multiplied by the price divided by the total volume.

As we can see here the price of an instrument can have multiple definitions depending on perspective. What we have chosen as our definition for the price of the stock and the future is the closing price. This is the most common definition when used for news reporting and in common discourse.

Defining the price of a commodity is a bit more of a nuanced topic. The price of a commodity is usually determined through futures contract. A definition of a futures contract can be given from Wikipedia "In finance, a futures contract (sometimes called futures) is a standardized legal contract to buy or sell something at a predetermined price for delivery at a specified time in the future, between parties not yet known to each other". With this in mind there is no "canonical" price of oil, but rather speculation on oil deliveries on different time frames. The definition we use for the price of oil on a given day is the close price of the oil future instrument with the most volume for a given day. This is usually the front month contract, which is the contract with the nearest expiration date. 

```{r}
# The data contains multiple different kind of futures contract, such as derivatives and butterfly spreads. This grep regex only takes out the regular future contract.
oil_daily = futures_price %>%  filter(grepl("^CL[A-Z][0-9]$", symbol))
oil_daily = oil_daily %>%  group_by(ts_event) %>% slice_max(volume,n=1,with_ties = FALSE) %>% ungroup() 
oil_daily = oil_daily %>% rename(date = ts_event,close_oil = close)

natural_gas_daily = futures_price %>%  filter(grepl("^NG[A-Z][0-9]$", symbol))
natural_gas_daily = natural_gas_daily %>%  group_by(ts_event) %>% slice_max(volume,n=1,with_ties = FALSE) %>% ungroup() 
natural_gas_daily = natural_gas_daily %>% rename(date = ts_event,close_gas = close)


```

Another important thing to note is that futures trading and stock trading are not traded at the same exchange and as such have different trading days. DOW is traded on NASDAQ and futures are traded on the CME/NYMEX calender. As a consequence there are more days were futures are traded in comparison to American stocks. We have not transformed the data in any way, we choose to instead only keep the futures observations that also occur on a trading day on NASDAQ This should be sufficient in the sense that the stock price can only affected when the trading of the stock is allowed to occur. A problem that occurs is that sometimes there are trading day for stocks when there is no trading for futures. To remedy this we replace the future price with the latest trading future price, which should be a sufficient remedy.
```{r}

dow_price = dow_price %>% left_join(dplyr::select(oil_daily,date,close_oil),by = "date")
dow_price = dow_price %>% left_join(dplyr::select(natural_gas_daily,date,close_gas),by = "date")
dow_price = dow_price %>% arrange(date) %>% fill(close_oil, .direction = "down")
dow_price = dow_price %>% arrange(date) %>% fill(close_gas, .direction = "down")
```

Here we can visualize the data. We do not assume that the time series have any particular peridiocity, the only thing we could speculate should exist is a positive trend where inflation and an tendency for the economy to grow should increase the prices. On the other hand all of these instruments are susceptible to a lot of variance stemming from global political shocks. Trumps tariffs, the war in Ukraine, OPECS price setting are all phenomenon that have a huge impact on the price that do not follow a particular pattern. 

Looking at the normalized graph we can see promising results. There seems to be a strong correlation between DOW and oil prices, and when the gas price flucuate so does it seem to affect the DOW price also. Interestingly the correlation seems to break later in the time series. This could be a consequence of a company specific incident that causes the stock to lose value.
```{r}
normalize <- function(x){ res <- x/max(x); res - res[[1]]} 
ggplot(dow_price, aes(date)) + ggtitle("Compared prices")+
    geom_line(aes(y = normalize( close_gas),colour ="Gas")) + geom_line(aes(y = normalize(close_oil),colour ="Oil")) +  geom_line(aes(y =  normalize(close),colour ="DOW"))
```

# Exponential smoothing

### Fitting a exponential smoother

For daliy stock data like the DOW stock, it is sutible to use a Holt-Winters method. Stock prices most often shows a trend but not as often a fixed daily seasonality that monthly data does. By using the Holt-Winters method we are able to visualize the underlying trend of the DOW´s development.

To begin with, in order to implement the Holt-Winters method we need to create a time series object.

```{r}
dow_ts <- ts(dow_price$close, start = 2019, frequency = 252)
```

By using the ts argument in R-studio we are able to convert the dataframe dow_price to a time series. The next step is to fit the Holt-Winters method with no seasonality and then the final step is to plot the observed data versus the fitted values of the Holt-Winters method.

```{r}
hw_dow <- HoltWinters(dow_ts, gamma = FALSE)

plot(hw_dow, main = "Plot 5: DOW Stock Price: Observed Values vs Fitted Values")
legend("topright",
       legend = c("Observed", "Fitted"),
       col = c("black", "red"),
       lty = 1,
       cex = 0.5)
```

As seen in plot 5 the fitted values follows the observed data very closley. This tends to be the case when smoothing volatile data, it often looks like the fitted values are lagging one step behind. This means that model estimates for the stock price for the DOW today has a heavy influence of the price from yesterday.

The smoothing parameters play a big part in the behavior of the red line. By looking at plot 5 we can determine that the level(a) must be high since the fitted line follows the observed line closely. This means that the model gives much weight to the most recent observations. We can see that the trend parameter is weak since the fitted values tend to follow the observed data rather than following a general direction.

### Rolling forecasts

In order to determine the predictive power of our HW model, rolling forecasts for the final 30 observations of the dataset is to be made. It makes it possible for the model to re-estimate the smoothing parameters as new data points are available.The code used is a function made so that the rolling forecasts are automatic.

```{r}
# Function for rolling forecasts
dow_rolling_forecasts <- function(data, n_test, horizon) {
  n_total <- length(data)
  preds <- numeric(n_test)
  
  for (i in 1:n_test) {
    train_end <- n_total - n_test + i - 1
    train_ts <- ts(data[1:train_end],
                   start = start(data),
                   frequency = frequency(data))
    train_fit <- HoltWinters(train_ts, gamma = FALSE)
    predicitions <- predict(train_fit, n.ahead = horizon)
    preds[i] <- predicitions[horizon]
  }
  final_results <- ts(preds,
             start = time(data)[n_total - n_test + 1],
             frequency = frequency(data))
  return(final_results)
}

```

Now that the function is defined we can make the rolling forecasts for the horizons h=1 and h=4 since our time series does not have any seasonality.

```{r warning=FALSE}
rf_dow_h1 <- dow_rolling_forecasts(dow_ts, n_test = 30, horizon = 1)
rf_dow_h4 <- dow_rolling_forecasts(dow_ts, n_test = 30, horizon = 4)
```

The next step in the process is to calculate the root mean sqaure error(RMSE) and mean absolute deviation(MAD). First we extract the last 30 observations of the observed data in order to calculate forecasting errors.

```{r}
# First extract the observed values
n_total <- length(dow_ts)
observed_values <- window(dow_ts, start = time(dow_ts)[n_total - 29])

# Secondly calculate the forecastning errors
fr_error_h1 <- observed_values - rf_dow_h1
fr_error_h4 <- observed_values - rf_dow_h4

# Last step is to calculate RMSE and MAD
rmse_h1 <- sqrt(mean(fr_error_h1^2))
rmse_h4 <- sqrt(mean(fr_error_h4^2))

mad_h1 <- mean(abs(fr_error_h1))
mad_h4 <- mean(abs(fr_error_h4))

results_matris <- matrix(c(rmse_h1, mad_h1,
                           rmse_h4, mad_h4),
                         nrow = 2, byrow = TRUE)
colnames(results_matris) <- c("RMSE", "MAD")
rownames(results_matris) <- c("h=1", "h=4")

result_table <- as.table(results_matris)
result_table
```

As table shows both the RMSE and MAD is lower for the shorter horizon. This shows that our exponential smoother is more accurate at predicting the the next day than predicting 4 days into the future.

In order to plot the predictions together with the observed data we had to alter the function shown earlier in the report.

```{r}
# Updated function
dow_rolling_intervals <- function(data, n_test, horizon, manual_alpha = 0.8) {
  n_total <- length(data)
# Matrix to store the three columns fit, lower and upper  
  preds_matrix <- matrix(NA, nrow = n_test, ncol = 3)
  
  for (i in 1:n_test) {
    train_end <- n_total - n_test + i - 1
# Manual alpha added
    train_ts <- ts(data[1:train_end],
                   start = start(data),
                   frequency = frequency(data))
    train_fit <- HoltWinters(train_ts, alpha = manual_alpha, gamma = FALSE)
# Intervals added
    int <- predict(train_fit, n.ahead = horizon, prediction.interval = TRUE)
    preds_matrix[i, ] <- int[horizon, ]
  }
  starting_time <- time(data)[n_total - n_test + 1]
  freq <- frequency(data)
 
   return(list(fit = ts(preds_matrix[,1], start = starting_time, frequency = freq),
              lower = ts(preds_matrix[,2], start = starting_time, frequency = freq),
              upper = ts(preds_matrix[,3], start = starting_time, frequency = freq)))
}
```

In the updated function there is an addition in the Holt-Winters function that uses a manual alpha value. The value chosen was 0.8 to ensure the model responds well to the high volatility of the DOW stock.

```{r}
results_h1 <- dow_rolling_intervals(dow_ts, n_test = 30, horizon = 1, manual_alpha = 0.8)
results_h4 <- dow_rolling_intervals(dow_ts, n_test = 30, horizon = 4, manual_alpha = 0.8)

start_test <- time(dow_ts)[length(dow_ts) -29]

ts.plot(dow_ts, results_h1$fit, results_h1$lower, results_h1$upper,
        col = c("black", "steelblue", "green", "green"),
        lty = c(1, 1, 2, 2),
        main = "Plot 6: Rolling Forecast Evalutation (h=1)",
        xlim = c(start_test - 0.05, start_test + 0.1),
        ylab = "Price")

legend("topright", legend = c("Observed", "Forecast", "95% CI"),
       col = c("black", "steelblue", "green", "green"), lty = c(1, 1, 2), cex = 0.8)

ts.plot(dow_ts, results_h4$fit, results_h4$lower, results_h4$upper,
        col = c("black", "steelblue", "green", "green"),
        lty = c(1, 1, 2, 2),
        main = "Plot 7: Rolling Forecast Evalutation (h=4)",
        xlim = c(start_test - 0.05, start_test + 0.1),
        ylab = "Price")

legend("topright", legend = c("Observed", "Forecast", "95% CI"),
       col = c("black", "steelblue", "green", "green"), lty = c(1, 1, 2), cex = 0.8)

```

Plot 6 and Plot 7 shows the rolling forecasts for horizons 1 and 4. With alpha being 0.8 the model is reactive to the volatility of the stock. As we could expect the interval for the prediction of the h=4 horizon the interval is wider than the h=1 horizon. It shows that the uncertainty increases as we make longer term forecast which was also show in the RMSE that rose from about 0.54 to 0.58.

### Comparing the Holt-Winters model with a Random Walk

```{r}
n_total <- length(dow_ts)
observed_values <- dow_ts[(n_total - 29):n_total]

# Random walk for h=1 and h=4
rw_h1_predi <- dow_ts[(n_total-30):(n_total-1)]
rw_h4_predi <- dow_ts[(n_total-33):(n_total-4)]

# Random walk errors
rw_error_h1 <- observed_values - rw_h1_predi
rw_error_h4 <- observed_values - rw_h4_predi

# Random walk RMSE
rw_rmse_h1 <- sqrt(mean(rw_error_h1^2))
rw_rmse_h4 <- sqrt(mean(rw_error_h4^2))

# Table to compare the results
RMSE_matrix <- matrix(c(rmse_h1, rw_rmse_h1,
                        rmse_h4, rw_rmse_h4),
                        nrow = 2, byrow = TRUE)
colnames(RMSE_matrix) <- c("HW RMSE", "RW RMSE")
rownames(RMSE_matrix) <- c("HW and RW h=1", "HW and RW h=4")
RMSE_table <- as.table(RMSE_matrix)
RMSE_table
```

By comparing the Holt-Winters model against the Random Walk model the results from the table shows that the Random Walk provides a slight improvement to forecasting on the horizon h=1. Though the Random Walks RMSE increases far more than the Holt-Winters model on the horizon h=4. This demonstrates that the Holt-Winters model is superior in catching the underlying trend of the DOW stock price over longer time periods compared to a Random Walk model.

# Stationarity 

The next step in the process is to look in to the stationarity of our time series. The purpose of determining stationarity is to look at the mean and the variance and see if they remain constant through out the time series. It is a cruical requirement for advanced time series models. In order to determine wheter the DOW index is stationary or not we will use an Augmented Dickey-Fuller test to see if the time series needs transformations in order to be stationary.

```{r}
plot(
  dow_ts,
  xlab = "",
  ylab = "Closing Price",
  main = " Plot 9: DOW stock price"
)
```

Looking at plot 9 we see that there is a slight trend to the time series and that in consequence tells us that it might not be stationary. To confirm this we use a ADF test that will show us if the time series is stationary or not.

```{r}
# ADF test with significance level of 0.05
adf.test(dow_ts)
```

The null hypothesis is that the series is non-stationary and the alternative hypothesis is that the time series is stationary. The p-value of the ADF test was 0.7273 which means we can not reject the null hypothesis since 0.7273\>0.05. This means that there is no statistical evidence that our time series is stationary and it requires transfromation in order to achive stationarity.

```{r warning=FALSE}
# First order differencing
dow_ts_stationarity <- diff(dow_ts)

# Complete a ADF test agian
adf.test(dow_ts_stationarity)
```

By using a first-order differencing we see that the p-value is smaller than 0.01. This means we are able to reject the null hypothesis and assume that our time series is now stationary which will enable further analysis of the Dow stock price.

```{r}
# The stationary time series plot
plot(dow_ts_stationarity,
     main = "Plot 8: Stationary Time Series of the DOW index",
     ylab = "Change in Price",
     col = "steelblue3")

# The correlogram
acf(dow_ts_stationarity,
    main = "Plot 10: Correlogram of The Stationary DOW Index")
```

In plot 10 we see that the transformed DOW index is stationary. After applying the first order differencing the unit root that was present during the first ADF test on the non-stationary time series have now been removed. Looking att the auto correlation function we see that by using a first order differencing the residual are now independently and identically distributed(IID), a crucial requirment for the continued analysis.

# ARMA model


```{r}
#Training data
n <- length(dow_ts)

train_data <- window(dow_ts,end = time(dow_ts)[n_total - 30])
```

```{r}
#Make it stationary

data_difference <- diff(train_data)
```


```{r warning = FALSE }

#Fit the ARMA model
arma_model <- arima(data_difference,order = c(1, 0, 1))

arma_model

```

```{r}
#fitted value
fit_value_difference <- data_difference - residuals(arma_model)

fitted_value <- train_data[-1] + fit_value_difference
```

```{r}
#Plot the data toghter with fitted values
plot(train_data,main = "Obs vs Fitted",ylab = "Closing Price",xlab = "")

lines(ts(fitted_value,start = time(train_data)[2],frequency = frequency(train_data)),col = "green",lwd = 2)

legend("topleft",legend = c("Observed", "Fitted"),col = c("black", "green"),lty = 1,cex = 1)
```
```{r}
residuals <- residuals(arma_model)

acf(residuals,main = "ACF TEST")

jarque.bera.test(residuals)
```
We see here that the reisduals does not seem to be autocorrelated, but they residuals are not normally distributed which affects our inference of the parameter estimates.
```{r}
#legnth and horizon
test <- 30
h1 <- 1
h4 <- 4
```

```{r}
#Rolling forecast
rolling_forecast <- function(data,horizon,test){
n <- length(data); f <- numeric(test)
for(i in 1:test){
ti <- window(data,end=time(data)[n-test+i-1])
d <- diff(ti)
a <- arima(d,order=c(1,0,1))
f[i] <- ti[length(ti)] + predict(a,n.ahead=horizon)$pred[horizon]}
  f
}
```

```{r warning = FALSE}
#forecast
forecast_h1 <- rolling_forecast(dow_ts, h1, test)
forecast_h4 <- rolling_forecast(dow_ts, h4, test)
```


```{r}

#Observed values
observed <- tail(dow_ts, test)

#RMSE and MAD
rmse_h1 <- sqrt(mean((observed - forecast_h1)^2))
mad_h1  <- mean(abs(observed - forecast_h1))
rmse_h4 <- sqrt(mean((observed - forecast_h4)^2))
mad_h4  <- mean(abs(observed - forecast_h4))

#D
#alpha and sigma
alpha <- 0.05
sigma <- sd(residuals(arma_model))

#plot for h1
upper_h1 <- forecast_h1 + qnorm(1 - alpha/2) * sigma
lower_h1 <- forecast_h1 - qnorm(1 - alpha/2) * sigma
plot(observed, type = "l", col = "black",main = "plot forecast h1",ylab = "Price", xlab = "")
lines(forecast_h1, col = "steelblue")
lines(upper_h1, col = "red", lty = 2)
lines(lower_h1, col = "red", lty = 2)

#plot for h4
upper_h4 <- forecast_h4 + qnorm(1 - alpha/2) * sigma
lower_h4 <- forecast_h4 - qnorm(1 - alpha/2) * sigma
plot(observed, type = "l", col = "black",main = "plot forecast h4",ylab = "Price", xlab = "")
lines(forecast_h4, col = "steelblue", lwd = 2)
lines(upper_h4, col = "red", lty = 2)
lines(lower_h4, col = "red", lty = 2)
```


```{r}
test <- 30
observed <- tail(dow_ts, test)

#forecast randomwalk (RW)
forecast_rw_h1 <- dow_ts[(length(dow_ts) - test):(length(dow_ts) - 1)]
forecast_rw_h4 <- dow_ts[(length(dow_ts) - test - 3):(length(dow_ts) - 4)]

#RMSE for ARMA and RW
rmse_arma_h1 <- sqrt(mean((observed - forecast_h1)^2))
rmse_rw_h1 <- sqrt(mean((observed - forecast_rw_h1)^2))
rmse_arma_h4 <- sqrt(mean((observed - forecast_h4)^2))
rmse_rw_h4 <- sqrt(mean((observed - forecast_rw_h4)^2))

#table for RMSE
rmse_table <- matrix(c(rmse_arma_h1, rmse_rw_h1,rmse_arma_h4, rmse_rw_h4),nrow = 2, byrow = TRUE)
colnames(rmse_table) <- c("RMSE ARMA", "RMSE RW")
rownames(rmse_table) <- c("h = 1", "h = 4")
rmse_table
```
The RMSE table in task e, shows that RMSE for ARMA and RW is nearly the same for h1. But for h4, ARMA is clearly much better. This suggests that the ARMA model is more useful for longer forecast horizons.

# Var model

```{r warning = FALSE}
# Var models require stationary data so we first make the data stationary and verify it with a adf test. 
oil_ts_stationary = diff(ts(dow_price$close_oil,start = 2019, frequency = 252))
gas_ts_stationary = diff(ts(dow_price$close_gas,start = 2019, frequency = 252))

adf.test(oil_ts_stationary)
adf.test(gas_ts_stationary)

DOW_var_model <- VAR(cbind(oil_price=oil_ts_stationary,gas_price = gas_ts_stationary,dow_price = dow_ts_stationarity), lag.max =20,ic="AIC",type = "const")

# We test for autocorrelation
portest(
cbind(DOW_var_model$varresult$oil_price$residuals, DOW_var_model$varresult$gas_price$residuals,DOW_var_model$varresult$dow_price$residuals),
test = "LjungBox"
)

#We test for normality here
jarque.bera.test((DOW_var_model$varresult$oil_price$residuals))
jarque.bera.test((DOW_var_model$varresult$gas_price$residuals))
jarque.bera.test((DOW_var_model$varresult$dow_price$residuals))

```
The residuals does not seem to be normally distributed. Our estimators should still be unbiased, but this will affect the confidence interval for our impulse reponse 

```{r}
imp_response <- irf(DOW_var_model, n.ahead = 20)
plot(imp_response)
```
Interestingly it seems that oil price does not seem to affect the price of the stock price in a significant manner, where a increase in oil prices seems to as a first consequence increase the price of the stock. We had predicted that the increase in oil prices should affect the margins of the buisness and therefore decrease the price. It could otherwise be speculated that a increase in oil is associated with an overall increase in economic activity or a higher demand for oil derived products such as plastic. 

Gas on the other hand seems to affect the DOW price to a higher degree where the price increases and then seems to decrease for a while. It is also interesting to see how much the predicted fluctuations of the oil price seems to come from a change in gas prices.

Here we evaluate the RMSE and MAD using a rolling forecast
```{r}
rolling_var_forecast <- function(
  all_data,
  test_size,
  horizon,
  forecast_variable_name,
  time_stamps
) {
  n_total <- nrow(all_data)
  max_index = test_size - horizon + 1
  actual_values = numeric(max_index)
  predictions <- numeric(max_index)
  
  prediction_intervalls = data.frame(
    time_stamp = numeric(max_index),
    forecast = numeric(max_index),
    lower = numeric(max_index),
    upper = numeric(max_index),
    actual = numeric(max_index)
  )
  
  
  for( i in 1:max_index)
  {
    train_end <- n_total - test_size + i - 1
    DOW_var_model <- VAR(all_data[1:train_end, ], lag.max =20,ic="AIC",type = "const")
    predict_return = predict(DOW_var_model, n.ahead = horizon,ci=0.95)$fcst[[forecast_variable_name]][horizon,c("fcst","lower","upper")]
    
    predictions[i] <- predict_return["fcst"]
    actual_values[i] <- all_data[train_end + horizon, forecast_variable_name]
    
    prediction_intervalls$time_stamp[i] = time_stamps[train_end + horizon]
    prediction_intervalls$forecast[i] = predict_return["fcst"]
    prediction_intervalls$lower[i] = predict_return["lower"]
    prediction_intervalls$upper[i] = predict_return["upper"]
    prediction_intervalls$actual[i] = all_data[train_end + horizon, forecast_variable_name]
  }

  errors <- actual_values - predictions

  list(
    RMSE = sqrt(mean(errors^2, na.rm = TRUE)),
    MAD  = mean(abs(errors), na.rm = TRUE),
    prediction_intervalls = prediction_intervalls
  )
}

data_test = cbind(oil_price=oil_ts_stationary,gas_price = gas_ts_stationary,dow_price = dow_ts_stationarity)

result_1 = rolling_var_forecast(data_test, 30,1,forecast_variable_name="dow_price",time(dow_ts_stationarity))
print(paste("RMSE",result_1$RMSE))
print(paste("MAD",result_1$MAD))
result_2 = rolling_var_forecast(data_test, 33,4,forecast_variable_name="dow_price",time(dow_ts_stationarity))
print(paste("RMSE",result_2$RMSE))
print(paste("MAD",result_2$MAD))


ggplot(result_1$prediction_intervalls, aes(x = time_stamp)) +
  geom_line(aes(y = actual, color = "Actual"), linewidth = 1) +
  geom_line(aes(y = forecast, color = "Forecast"), linewidth = 1) +
  geom_line(aes(y = forecast, color = "Forecast"), linewidth = 1) +
  geom_ribbon(aes(ymin = lower, ymax = upper, fill = "Prediction Interval"), alpha = 0.2) +
  scale_color_manual(name = "Series", values = c("Actual" = "black", "Forecast" = "blue")) +
  scale_fill_manual(name = "", values = c("Prediction Interval" = "red")) +
  ggtitle("Rolling Forecasts") +
  theme_minimal()


ggplot(result_2$prediction_intervalls, aes(x = time_stamp)) +
  geom_line(aes(y = actual, color = "Actual"), linewidth = 1) +
  geom_line(aes(y = forecast, color = "Forecast"), linewidth = 1) +
  geom_ribbon(aes(ymin = lower, ymax = upper, fill = "Prediction Interval"), alpha = 0.2) +
  scale_color_manual(name = "Series", values = c("Actual" = "black", "Forecast" = "blue")) +
  scale_fill_manual(name = "", values = c("Prediction Interval" = "red")) +
  ggtitle("Rolling Forecast h=4") +
  theme_minimal()

```

This is then compared to a random walk. Here we can see that the var model outperforms the random walk, if only marginally, during a time horizon of 1, while being substantially better at a horizon of 4.  
```{r}
# Table to compare the results
RMSE_matrix <- matrix(c(result_1$RMSE, rw_rmse_h1,
                        result_2$RMSE, rw_rmse_h4),
                        nrow = 2, byrow = TRUE)
colnames(RMSE_matrix) <- c("VAR RMSE", "RW RMSE")
rownames(RMSE_matrix) <- c("VAR and RW h=1", "VAR and RW h=4")
RMSE_table <- as.table(RMSE_matrix)
RMSE_table
```
The var model compared to all other models does not really provide substantially better results. There is more to be said if it should even be assumed that gas and oil prices affect each other. A Var model could potentially be more suitable given that a larger segment of the market was captured in this analysis where there is a reasonable assumption that for example the whole segment of plastic producers might affect the price of oil rather than an individual company. 
